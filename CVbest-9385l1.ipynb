{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "import lightgbm as lgb\n",
    "import optuna.integration.lightgbm as lgbopt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import umap\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "pd.set_option(\"display.precision\", 8)\n",
    "\n",
    "from functools import partial\n",
    "import scipy as sp\n",
    "\n",
    "import os\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import pywt \n",
    "from statsmodels.robust import mad\n",
    "\n",
    "import scipy\n",
    "from scipy import signal\n",
    "from scipy.signal import butter, deconvolve, find_peaks, peak_widths, peak_prominences\n",
    "\n",
    "from numpy.fft import *\n",
    "\n",
    "import time\n",
    "import math\n",
    "from numba import jit\n",
    "from math import log, floor\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "import itertools\n",
    "import warnings\n",
    "import time\n",
    "import pywt\n",
    "import os\n",
    "import gc\n",
    "from tsfresh import extract_features\n",
    "\n",
    "\n",
    "train = pd.read_csv('/Users/siero5335/channel/train.csv')\n",
    "test = pd.read_csv('/Users/siero5335/channel/test.csv')\n",
    "train2 = pd.read_csv('/Users/siero5335/channel/train2.csv')\n",
    "test2 = pd.read_csv('/Users/siero5335/channel/test2.csv')\n",
    "\n",
    "\n",
    "sample_submission = pd.read_csv('/Users/siero5335/channel/sample_submission.csv')\n",
    "\n",
    "train2 = train2.iloc[:,1]\n",
    "test2 = test2.iloc[:,1]\n",
    "\n",
    "train = pd.concat([train, train2], axis=1)\n",
    "test = pd.concat([test, test2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6070"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.drop('signal', axis = 1)\n",
    "test = test.drop('signal', axis = 1)\n",
    "\n",
    "train = train.rename(columns={'signal_chris': 'signal'})\n",
    "test = test.rename(columns={'signal_chris': 'signal'})\n",
    "\n",
    "del train2, test2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Extraction:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "extracted_features_train = extract_features(train, column_id=\"time\")\n",
    "extracted_features_test = extract_features(test, column_id=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maddest(d, axis=None):\n",
    "    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\n",
    "\n",
    "def high_pass_filter(x, low_cutoff=1000, sample_rate=10000):\n",
    "\n",
    "    nyquist = 0.5 * sample_rate\n",
    "    norm_low_cutoff = low_cutoff / nyquist\n",
    "    print(norm_low_cutoff)\n",
    "    sos = butter(10, Wn=[norm_low_cutoff], btype='highpass', output='sos')\n",
    "    filtered_sig = signal.sosfilt(sos, x)\n",
    "\n",
    "    return filtered_sig\n",
    "\n",
    "def denoise_signal( x, wavelet='db4', level=1):\n",
    "    \n",
    "    coeff = pywt.wavedec( x, wavelet, mode=\"per\" )\n",
    "    sigma = (1/0.6745) * maddest( coeff[-level] )\n",
    "    uthresh = sigma * np.sqrt( 2*np.log( len( x ) ) )\n",
    "    coeff[1:] = ( pywt.threshold( i, value=uthresh, mode='hard' ) for i in coeff[1:] )\n",
    "    return pywt.waverec( coeff, wavelet, mode='per' )\n",
    "\n",
    "train['signal_wave'] = denoise_signal(train['signal'])\n",
    "test['signal_wave'] = denoise_signal(test['signal'])\n",
    "\n",
    "def filter_signal(signal, threshold=1e8):\n",
    "    fourier = rfft(signal)\n",
    "    frequencies = rfftfreq(signal.size, d=20e-3/signal.size)\n",
    "    fourier[frequencies > threshold] = 0\n",
    "    return irfft(fourier)\n",
    "\n",
    "train['signal_FFT_1e5'] = filter_signal(train['signal'], threshold=5e3)\n",
    "test['signal_FFT_1e5'] = filter_signal(test['signal'], threshold=5e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _embed(x, order=3, delay=1):\n",
    "    N = len(x)\n",
    "    if order * delay > N:\n",
    "        raise ValueError(\"Error: order * delay should be lower than x.size\")\n",
    "    if delay < 1:\n",
    "        raise ValueError(\"Delay has to be at least 1.\")\n",
    "    if order < 2:\n",
    "        raise ValueError(\"Order has to be at least 2.\")\n",
    "    Y = np.zeros((order, N - (order - 1) * delay))\n",
    "    for i in range(order):\n",
    "        Y[i] = x[i * delay:i * delay + Y.shape[1]]\n",
    "    return Y.T\n",
    "\n",
    "all = ['perm_entropy', 'spectral_entropy', 'svd_entropy', 'app_entropy',\n",
    "       'sample_entropy']\n",
    "\n",
    "\n",
    "def perm_entropy(x, order=3, delay=1, normalize=False):\n",
    "    x = np.array(x)\n",
    "    ran_order = range(order)\n",
    "    hashmult = np.power(order, ran_order)\n",
    "    # Embed x and sort the order of permutations\n",
    "    sorted_idx = _embed(x, order=order, delay=delay).argsort(kind='quicksort')\n",
    "    # Associate unique integer to each permutations\n",
    "    hashval = (np.multiply(sorted_idx, hashmult)).sum(1)\n",
    "    # Return the counts\n",
    "    _, c = np.unique(hashval, return_counts=True)\n",
    "    # Use np.true_divide for Python 2 compatibility\n",
    "    p = np.true_divide(c, c.sum())\n",
    "    pe = -np.multiply(p, np.log2(p)).sum()\n",
    "    if normalize:\n",
    "        pe /= np.log2(factorial(order))\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _log_n(min_n, max_n, factor):\n",
    "    max_i = int(floor(log(1.0 * max_n / min_n) / log(factor)))\n",
    "    ns = [min_n]\n",
    "    for i in range(max_i + 1):\n",
    "        n = int(floor(min_n * (factor ** i)))\n",
    "        if n > ns[-1]:\n",
    "            ns.append(n)\n",
    "    return np.array(ns, dtype=np.int64)\n",
    "\n",
    "def _higuchi_fd(x, kmax):\n",
    "    n_times = x.size\n",
    "    lk = np.empty(kmax)\n",
    "    x_reg = np.empty(kmax)\n",
    "    y_reg = np.empty(kmax)\n",
    "    for k in range(1, kmax + 1):\n",
    "        lm = np.empty((k,))\n",
    "        for m in range(k):\n",
    "            ll = 0\n",
    "            n_max = floor((n_times - m - 1) / k)\n",
    "            n_max = int(n_max)\n",
    "            for j in range(1, n_max):\n",
    "                ll += abs(x[m + j * k] - x[m + (j - 1) * k])\n",
    "            ll /= k\n",
    "            ll *= (n_times - 1) / (k * n_max)\n",
    "            lm[m] = ll\n",
    "        # Mean of lm\n",
    "        m_lm = 0\n",
    "        for m in range(k):\n",
    "            m_lm += lm[m]\n",
    "        m_lm /= k\n",
    "        lk[k - 1] = m_lm\n",
    "        x_reg[k - 1] = log(1. / k)\n",
    "        y_reg[k - 1] = log(m_lm)\n",
    "    higuchi, _ = _linear_regression(x_reg, y_reg)\n",
    "    return higuchi\n",
    "\n",
    "\n",
    "def higuchi_fd(x, kmax=10):\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    kmax = int(kmax)\n",
    "    return _higuchi_fd(x, kmax)\n",
    "\n",
    "def _linear_regression(x, y):\n",
    "    n_times = x.size\n",
    "    sx2 = 0\n",
    "    sx = 0\n",
    "    sy = 0\n",
    "    sxy = 0\n",
    "    for j in range(n_times):\n",
    "        sx2 += x[j] ** 2\n",
    "        sx += x[j]\n",
    "        sxy += x[j] * y[j]\n",
    "        sy += y[j]\n",
    "    den = n_times * sx2 - (sx ** 2)\n",
    "    num = n_times * sxy - sx * sy\n",
    "    slope = num / den\n",
    "    intercept = np.mean(y) - slope * np.mean(x)\n",
    "    return slope, intercept\n",
    "\n",
    "def katz_fd(x):\n",
    "    x = np.array(x)\n",
    "    dists = np.abs(np.ediff1d(x))\n",
    "    ll = dists.sum()\n",
    "    ln = np.log10(np.divide(ll, dists.mean()))\n",
    "    aux_d = x - x[0]\n",
    "    d = np.max(np.abs(aux_d[1:]))\n",
    "    return np.divide(ln, np.add(ln, np.log10(np.divide(d, ll))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        if col!='open_channels':\n",
    "            col_type = df[col].dtypes\n",
    "            if col_type in numerics:\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "                if str(col_type)[:3] == 'int':\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                        df[col] = df[col].astype(np.int64)  \n",
    "                else:\n",
    "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                        df[col] = df[col].astype(np.float16)\n",
    "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p1( x : pd.Series) -> pd.Series : return x.quantile(0.01)\n",
    "def p5(x : pd.Series) -> pd.Series : return x.quantile(0.05)\n",
    "def p95( x : pd.Series) -> pd.Series : return x.quantile(0.95)\n",
    "def p99(x : pd.Series) -> pd.Series : return x.quantile(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore = lambda x: (x - x.mean()) / x.std()\n",
    "\n",
    "window_sizes = [10, 50, 100, 500, 1000, 5000]\n",
    "\n",
    "for window in window_sizes:\n",
    "    train[\"rolling_mean_\" + str(window)] = train['signal'].rolling(window=window).mean()\n",
    "    train[\"rolling_std_\" + str(window)] = train['signal'].rolling(window=window).std()\n",
    "    train[\"rolling_min_\" + str(window)] = train['signal'].rolling(window=window).min()\n",
    "    train[\"rolling_max_\" + str(window)] = train['signal'].rolling(window=window).max()\n",
    "    \n",
    "    train[\"rolling_min_max_ratio_\" + str(window)] = train[\"rolling_min_\" + str(window)] / train[\"rolling_max_\" + str(window)]\n",
    "    train[\"rolling_min_max_diff_\" + str(window)] = train[\"rolling_max_\" + str(window)] - train[\"rolling_min_\" + str(window)]\n",
    "    \n",
    "    a = (train['signal'] - train['rolling_min_' + str(window)]) / (train['rolling_max_' + str(window)] - train['rolling_min_' + str(window)])\n",
    "    train[\"norm_\" + str(window)] = a * (np.floor(train['rolling_max_' + str(window)]) - np.ceil(train['rolling_min_' + str(window)]))\n",
    "    \n",
    "train = train.replace([np.inf, -np.inf], np.nan)    \n",
    "train.fillna(0, inplace=True)\n",
    "    \n",
    "for window in window_sizes:\n",
    "    test[\"rolling_mean_\" + str(window)] = test['signal'].rolling(window=window).mean()\n",
    "    test[\"rolling_std_\" + str(window)] = test['signal'].rolling(window=window).std()\n",
    "    test[\"rolling_min_\" + str(window)] = test['signal'].rolling(window=window).min()\n",
    "    test[\"rolling_max_\" + str(window)] = test['signal'].rolling(window=window).max()\n",
    "    \n",
    "    test[\"rolling_min_max_ratio_\" + str(window)] = test[\"rolling_min_\" + str(window)] / test[\"rolling_max_\" + str(window)]\n",
    "    test[\"rolling_min_max_diff_\" + str(window)] = test[\"rolling_max_\" + str(window)] - test[\"rolling_min_\" + str(window)]\n",
    "\n",
    "    \n",
    "    a = (test['signal'] - test['rolling_min_' + str(window)]) / (test['rolling_max_' + str(window)] - test['rolling_min_' + str(window)])\n",
    "    test[\"norm_\" + str(window)] = a * (np.floor(test['rolling_max_' + str(window)]) - np.ceil(test['rolling_min_' + str(window)]))\n",
    "\n",
    "test = test.replace([np.inf, -np.inf], np.nan)    \n",
    "test.fillna(0, inplace=True)\n",
    "    \n",
    "    \n",
    "def features(df):\n",
    "    df = df.sort_values(by=['time']).reset_index(drop=True)\n",
    "    df.index = ((df.time * 10_000) - 1).values\n",
    "    df['batch'] = df.index // 50_000\n",
    "    df['batch_index'] = df.index  - (df.batch * 50_000)\n",
    "    df['batch_slices'] = df['batch_index']  // 5_000\n",
    "    df['batch_slices2'] = df.apply(lambda r: '_'.join([str(r['batch']).zfill(3), str(r['batch_slices']).zfill(3)]), axis=1)\n",
    "    \n",
    "    for c in ['batch','batch_slices2']:\n",
    "        d = {}\n",
    "                                            \n",
    "        d['mean'+c] = df.groupby([c])['signal'].mean()\n",
    "        d['median'+c] = df.groupby([c])['signal'].median()\n",
    "        d['max'+c] = df.groupby([c])['signal'].max()\n",
    "        d['min'+c] = df.groupby([c])['signal'].min()\n",
    "        d['std'+c] = df.groupby([c])['signal'].std()\n",
    "        d['skew'+c] = df.groupby([c])['signal'].skew()\n",
    "        \n",
    "        d['q1'+c] = df.groupby([c])['signal'].apply(lambda x:p1(x))\n",
    "        d['q5'+c] = df.groupby([c])['signal'].apply(lambda x:p5(x))\n",
    "        d['q95'+c] = df.groupby([c])['signal'].apply(lambda x:p95(x))       \n",
    "        d['q99'+c] = df.groupby([c])['signal'].apply(lambda x:p99(x))                   \n",
    "        \n",
    "        d['signal_batch'+c] = df.groupby([c])['signal'].transform(zscore)\n",
    "        d['perm'+c] = df.groupby([c])['signal'].apply(lambda x:perm_entropy(x))\n",
    "        d['higuchi'+c] = df.groupby([c])['signal'].apply(lambda x:higuchi_fd(x))\n",
    "        d['katz'+c] = df.groupby([c])['signal'].apply(lambda x:katz_fd(x))\n",
    "                \n",
    "        d['mean_abs_chg'+c] = df.groupby([c])['signal'].apply(lambda x: np.mean(np.abs(np.diff(x))))\n",
    "        d['abs_max'+c] = df.groupby([c])['signal'].apply(lambda x: np.max(np.abs(x)))\n",
    "        d['abs_min'+c] = df.groupby([c])['signal'].apply(lambda x: np.min(np.abs(x)))\n",
    "        for v in d:\n",
    "            df[v] = df[c].map(d[v].to_dict())\n",
    "        df['range'+c] = df['max'+c] - df['min'+c]\n",
    "        df['maxtomin'+c] = df['max'+c] / df['min'+c]\n",
    "        df['abs_avg'+c] = (df['abs_min'+c] + df['abs_max'+c]) / 2\n",
    "    \n",
    "    #add shifts\n",
    "    df['signal_shift_+1'] = [0,] + list(df['signal'].values[:-1])\n",
    "    df['signal_shift_-1'] = list(df['signal'].values[1:]) + [0]\n",
    "    for i in df[df['batch_index']==0].index:\n",
    "        df['signal_shift_+1'][i] = np.nan\n",
    "    for i in df[df['batch_index']==49999].index:\n",
    "        df['signal_shift_-1'][i] = np.nan\n",
    "        \n",
    "    df['signal_shift_wave_+1'] = [0,] + list(df['signal_wave'].values[:-1])\n",
    "    df['signal_shift_wave_-1'] = list(df['signal_wave'].values[1:]) + [0]\n",
    "    for i in df[df['batch_index']==0].index:\n",
    "        df['signal_shift_wave_+1'][i] = np.nan\n",
    "    for i in df[df['batch_index']==49999].index:\n",
    "        df['signal_shift_wave_-1'][i] = np.nan\n",
    "        \n",
    "    \n",
    "    # add shifts_2 \n",
    "    df['signal_shift_+2'] = [0,] + [1,] + list(df['signal'].values[:-2])\n",
    "    df['signal_shift_-2'] = list(df['signal'].values[2:]) + [0] + [1]\n",
    "    for i in df[df['batch_index']==0].index:\n",
    "        df['signal_shift_+2'][i] = np.nan\n",
    "    for i in df[df['batch_index']==1].index:\n",
    "        df['signal_shift_+2'][i] = np.nan\n",
    "    for i in df[df['batch_index']==49999].index:\n",
    "        df['signal_shift_-2'][i] = np.nan\n",
    "    for i in df[df['batch_index']==49998].index:\n",
    "        df['signal_shift_-2'][i] = np.nan\n",
    "        \n",
    "    # add shifts_3\n",
    "    df['signal_shift_+3'] = [0,] + [1,]+ [2,] + list(df['signal'].values[:-3])\n",
    "    df['signal_shift_-3'] = list(df['signal'].values[3:]) + [0] + [1] + [2]\n",
    "    for i in df[df['batch_index']==0].index:\n",
    "        df['signal_shift_+3'][i] = np.nan\n",
    "    for i in df[df['batch_index']==1].index:\n",
    "        df['signal_shift_+3'][i] = np.nan\n",
    "    for i in df[df['batch_index']==2].index:\n",
    "        df['signal_shift_+3'][i] = np.nan\n",
    "    for i in df[df['batch_index']==49999].index:\n",
    "        df['signal_shift_-3'][i] = np.nan\n",
    "    for i in df[df['batch_index']==49998].index:\n",
    "        df['signal_shift_-3'][i] = np.nan    \n",
    "    for i in df[df['batch_index']==49997].index:\n",
    "        df['signal_shift_-3'][i] = np.nan       \n",
    "    \n",
    "        \n",
    "    for c in [c1 for c1 in df.columns if c1 not in ['time', 'signal', 'open_channels', 'batch', 'batch_index', 'batch_slices', 'batch_slices2']]:\n",
    "        df[c+'_msignal'] = df[c] - df['signal']\n",
    "        \n",
    "    return df\n",
    "\n",
    "train = features(train)\n",
    "test = features(test)\n",
    "\n",
    "\n",
    "train.fillna(0, inplace=True)\n",
    "test.fillna(0, inplace=True)\n",
    "\n",
    "train_y = train['open_channels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train, extracted_features_train], axis = 1)\n",
    "test = pd.concat([test, extracted_features_test], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [c for c in train.columns if c not in ['time', 'open_channels', 'batch', 'batch_index', 'batch_slices', 'batch_slices2']]\n",
    "\n",
    "train = train[col]\n",
    "test = test[col]\n",
    "\n",
    "n_fold = 5\n",
    "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.to_csv('/Users/siero5335/channel/train_mod.csv')\n",
    "#test.to_csv('/Users/siero5335/channel/test_mod.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = VarianceThreshold(threshold=0.1)\n",
    "sel.fit(train)\n",
    "\n",
    "train = train.loc[:, sel.get_support()]\n",
    "test = test.loc[:, sel.get_support()]\n",
    "\n",
    "threshold = 0.8\n",
    "\n",
    "feat_corr = set()\n",
    "corr_matrix = train.corr()\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "            feat_name = corr_matrix.columns[i]\n",
    "            feat_corr.add(feat_name)\n",
    "\n",
    "print(len(set(feat_corr)))\n",
    "# -> 658\n",
    "\n",
    "train.drop(labels=feat_corr, axis='columns', inplace=True)\n",
    "test.drop(labels=feat_corr, axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "train_features, valid_features, train_labels, valid_labels = model_selection.train_test_split(train, train_y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains = lgb.Dataset(train_features, train_labels)\n",
    "valids = lgb.Dataset(valid_features, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'learning_rate': 0.3, \n",
    "    'metric': 'l1',\n",
    "    'bagging_fraction': 1.0,\n",
    "    'bagging_freq': 0\n",
    "}\n",
    "best_params, history = {}, []\n",
    "model = lgbopt.train(params, trains, valid_sets=valids,\n",
    "                    verbose_eval=False,\n",
    "                    num_boost_round=1000,\n",
    "                    early_stopping_rounds=10,\n",
    "                    best_params=best_params,\n",
    "                    tuning_history=history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params.update(learning_rate = 0.01, bagging_fraction = 1.0, bagging_freq = 0,  metric = 'mae', random_state= 71, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = np.zeros(len(train))\n",
    "prediction = np.zeros(len(test))\n",
    "scores = []\n",
    "\n",
    "params = best_params\n",
    "\n",
    "for fold_n, (train_index, valid_index) in enumerate(folds.split(train, train_y)):\n",
    "    print('Fold', fold_n, 'started at', time.ctime())\n",
    "    X_train, X_valid = train.iloc[train_index], train.iloc[valid_index]\n",
    "    y_train, y_valid = train_y.iloc[train_index], train_y.iloc[valid_index]\n",
    "    \n",
    "    model = lgb.LGBMRegressor(**params, n_estimators = 20000)\n",
    "    model.fit(X_train, y_train, \n",
    "            eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',\n",
    "            verbose=100, early_stopping_rounds=100)\n",
    "\n",
    "    y_pred_valid = model.predict(X_valid)\n",
    "    y_pred = model.predict(test, num_iteration=model.best_iteration_)\n",
    "\n",
    "    oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "    scores.append(mean_absolute_error(y_valid, y_pred_valid))\n",
    "\n",
    "    prediction += y_pred\n",
    "\n",
    "prediction /= n_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedRounder(object):\n",
    "    \"\"\"\n",
    "    An optimizer for rounding thresholds\n",
    "    to maximize F1 (Macro) score\n",
    "    # https://www.kaggle.com/naveenasaithambi/optimizedrounder-improved\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _f1_loss(self, coef, X, y):\n",
    "        \"\"\"\n",
    "        Get loss according to\n",
    "        using current coefficients\n",
    "        \n",
    "        :param coef: A list of coefficients that will be used for rounding\n",
    "        :param X: The raw predictions\n",
    "        :param y: The ground truth labels\n",
    "        \"\"\"\n",
    "        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "        return -f1_score(y, X_p, average = 'macro')\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Optimize rounding thresholds\n",
    "        \n",
    "        :param X: The raw predictions\n",
    "        :param y: The ground truth labels\n",
    "        \"\"\"\n",
    "        loss_partial = partial(self._f1_loss, X=X, y=y)\n",
    "        initial_coef = [0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        \"\"\"\n",
    "        Make predictions with specified thresholds\n",
    "        \n",
    "        :param X: The raw predictions\n",
    "        :param coef: A list of coefficients that will be used for rounding\n",
    "        \"\"\"\n",
    "        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "\n",
    "    def coefficients(self):\n",
    "        \"\"\"\n",
    "        Return the optimized coefficients\n",
    "        \"\"\"\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optR = OptimizedRounder()\n",
    "optR.fit(oof.reshape(-1,), train_y)\n",
    "coefficients = optR.coefficients()\n",
    "print(coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_preds = optR.predict(oof.reshape(-1,), coefficients)\n",
    "f1_score(train_y, opt_preds, average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction[prediction <= coefficients[0]] = 0\n",
    "prediction[np.where(np.logical_and(prediction > coefficients[0], prediction <= coefficients[1]))] = 1\n",
    "prediction[np.where(np.logical_and(prediction > coefficients[1], prediction <= coefficients[2]))] = 2\n",
    "prediction[np.where(np.logical_and(prediction > coefficients[2], prediction <= coefficients[3]))] = 3\n",
    "prediction[np.where(np.logical_and(prediction > coefficients[3], prediction <= coefficients[4]))] = 4\n",
    "prediction[np.where(np.logical_and(prediction > coefficients[4], prediction <= coefficients[5]))] = 5\n",
    "prediction[np.where(np.logical_and(prediction > coefficients[5], prediction <= coefficients[6]))] = 6\n",
    "prediction[np.where(np.logical_and(prediction > coefficients[6], prediction <= coefficients[7]))] = 7\n",
    "prediction[np.where(np.logical_and(prediction > coefficients[7], prediction <= coefficients[8]))] = 8\n",
    "prediction[np.where(np.logical_and(prediction > coefficients[8], prediction <= coefficients[9]))] = 9\n",
    "prediction[prediction > coefficients[9]] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['open_channels'] = prediction.astype(np.int)\n",
    "sample_submission.to_csv('maeopt13.csv', index=False, float_format='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(model,importance_type='gain', max_num_features=1000, figsize=(20, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('/Users/siero5335/channel/train_mod.csv')\n",
    "test.to_csv('/Users/siero5335/channel/test_mod.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
